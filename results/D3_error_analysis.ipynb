{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pandas import DataFrame, concat, set_option\n",
    "\n",
    "set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "test_data_file = '../data/sarc/dev-comments/balanced.json'\n",
    "model_out_file = 'D3_scores.out'\n",
    "\n",
    "with open(test_data_file, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open(model_out_file, 'r') as f:\n",
    "    # save output file in dict\n",
    "    model_out = {'pred':[], 'gold':[], 'prob_negative':[], 'prob_positive':[]}\n",
    "\n",
    "    for line in f:\n",
    "        lst = line.replace(',', '').split()\n",
    "        # index labels and probabilities from each line, store in dict\n",
    "        model_out['pred'].append(lst[1])\n",
    "        model_out['gold'].append(lst[3])\n",
    "        model_out['prob_negative'].append(lst[5])\n",
    "        model_out['prob_positive'].append(lst[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE\n",
    "\n",
    "df1 = DataFrame.from_dict(model_out)\n",
    "df2 = DataFrame.from_records(test_data)\n",
    "\n",
    "table = concat([df2['response'], df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 20 of the incorrectly classified examples\n",
    "\n",
    "print(table.loc[table['pred'] != table['gold']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore false positives (model label = sarcastic, gold label = not sarcastic)\n",
    "\n",
    "false_pos = table.loc[(table['pred'] == '1') & (table['gold'] == '0')]\n",
    "print(false_pos.head(20))\n",
    "print('count false positives:', false_pos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore false negatives (model label = not sarcastic, gold label = sarcastic)\n",
    "\n",
    "false_neg = table.loc[(table['pred'] == '0') & (table['gold'] == '1')]\n",
    "print(false_neg.head(20))\n",
    "print('count false negatives:', false_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances the model was uncertain about"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
